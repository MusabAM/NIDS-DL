{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQC Inference and Analysis - NSL-KDD\n",
    "\n",
    "This notebook loads trained Variational Quantum Classifier models and performs comprehensive evaluation and analysis.\n",
    "\n",
    "**Tasks:**\n",
    "- Load trained VQC models\n",
    "- Evaluate on NSL-KDD test set\n",
    "- Generate detailed performance metrics\n",
    "- Compare with classical baselines\n",
    "- Analyze model behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# PennyLane\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: PennyLane not installed\")\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "\n",
    "# Import custom models\n",
    "from src.models.quantum.pennylane_models import HybridQuantumClassifier\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = project_root / 'data' / 'raw' / 'NSL-KDD' / 'NSL-KDD Dataset'\n",
    "MODELS_PATH = project_root / 'results' / 'models' / 'quantum'\n",
    "LOGS_PATH = project_root / 'results' / 'logs'\n",
    "FIGURES_PATH = project_root / 'results' / 'figures'\n",
    "\n",
    "# List available models\n",
    "print(\"Available VQC models:\")\n",
    "model_files = sorted(MODELS_PATH.glob('vqc_hybrid_nsl_kdd_*.pt'))\n",
    "for i, model_file in enumerate(model_files):\n",
    "    print(f\"  {i+1}. {model_file.name}\")\n",
    "\n",
    "if not model_files:\n",
    "    print(\"  No VQC models found. Please run 01_vqc_nsl_kdd.ipynb first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most recent model (or specify index)\n",
    "if model_files:\n",
    "    MODEL_FILE = model_files[-1]  # Most recent\n",
    "    print(f\"\\nUsing model: {MODEL_FILE.name}\")\n",
    "    \n",
    "    # Extract timestamp from filename\n",
    "    timestamp = MODEL_FILE.stem.split('_')[-2] + '_' + MODEL_FILE.stem.split('_')[-1]\n",
    "    \n",
    "    # Find corresponding preprocessing file\n",
    "    PREPROCESSING_FILE = MODELS_PATH / f'vqc_preprocessing_{timestamp}.pkl'\n",
    "    \n",
    "    if PREPROCESSING_FILE.exists():\n",
    "        print(f\"Preprocessing file: {PREPROCESSING_FILE.name}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Preprocessing file not found: {PREPROCESSING_FILE.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing pipeline\n",
    "with open(PREPROCESSING_FILE, 'rb') as f:\n",
    "    preprocessing = pickle.load(f)\n",
    "\n",
    "scaler = preprocessing['scaler']\n",
    "pca = preprocessing['pca']\n",
    "feature_cols = preprocessing['feature_cols']\n",
    "n_qubits = preprocessing['n_qubits']\n",
    "\n",
    "print(f\"Preprocessing pipeline loaded:\")\n",
    "print(f\"  Original features: {len(feature_cols)}\")\n",
    "print(f\"  PCA components: {n_qubits}\")\n",
    "print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = torch.load(MODEL_FILE, map_location=device)\n",
    "\n",
    "# Extract configurations\n",
    "model_config = checkpoint['model_config']\n",
    "training_config = checkpoint['training_config']\n",
    "test_metrics = checkpoint.get('test_metrics', {})\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTraining Info:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model\n",
    "model = HybridQuantumClassifier(**model_config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSL-KDD columns\n",
    "COLUMNS = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
    "]\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(DATA_PATH / 'KDDTest+.txt', header=None, names=COLUMNS)\n",
    "test_df['binary_label'] = (test_df['label'] != 'normal').astype(int)\n",
    "\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(\"\\nTest distribution:\")\n",
    "print(test_df['binary_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "test_encoded = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "\n",
    "# Ensure all feature columns are present\n",
    "for col in feature_cols:\n",
    "    if col not in test_encoded.columns:\n",
    "        test_encoded[col] = 0\n",
    "\n",
    "# Extract features\n",
    "X_test = test_encoded[feature_cols].values\n",
    "y_test = test_df['binary_label'].values\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Test data shape after preprocessing: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "X_test_tensor = torch.FloatTensor(X_test_pca)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "inference_times = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in tqdm(test_loader, desc='Inference'):\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        outputs = model(X_batch)\n",
    "        inference_times.append(time.time() - start_time)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Inference statistics\n",
    "total_inference_time = sum(inference_times)\n",
    "avg_time_per_batch = np.mean(inference_times)\n",
    "avg_time_per_sample = total_inference_time / len(all_preds)\n",
    "\n",
    "print(f\"\\nInference Statistics:\")\n",
    "print(f\"Total inference time: {total_inference_time:.2f} seconds\")\n",
    "print(f\"Average time per batch: {avg_time_per_batch:.4f} seconds\")\n",
    "print(f\"Average time per sample: {avg_time_per_sample*1000:.2f} ms\")\n",
    "print(f\"Throughput: {len(all_preds)/total_inference_time:.2f} samples/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "roc_auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYBRID VQC - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(all_labels, all_preds, \n",
    "                          target_names=['Normal', 'Attack'], \n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"False Positives: {cm[0,1]:,}\")\n",
    "print(f\"False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"True Positives:  {cm[1,1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Normal', 'Attack'],\n",
    "            yticklabels=['Normal', 'Attack'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix - Hybrid VQC (Inference)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'vqc_inference_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probs[:, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'VQC (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Hybrid VQC', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'vqc_inference_roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(all_labels, all_probs[:, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_curve, precision_curve, linewidth=2, label='VQC')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'vqc_inference_pr_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# For Normal samples\n",
    "normal_probs = all_probs[all_labels == 0, 0]\n",
    "ax1.hist(normal_probs, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.axvline(0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax1.set_xlabel('Confidence (Probability of Normal)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Prediction Confidence for Normal Samples', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# For Attack samples\n",
    "attack_probs = all_probs[all_labels == 1, 1]\n",
    "ax2.hist(attack_probs, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "ax2.axvline(0.5, color='blue', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax2.set_xlabel('Confidence (Probability of Attack)', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Prediction Confidence for Attack Samples', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'vqc_inference_confidence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify misclassified samples\n",
    "false_positives = (all_labels == 0) & (all_preds == 1)\n",
    "false_negatives = (all_labels == 1) & (all_preds == 0)\n",
    "\n",
    "print(\"Error Analysis:\")\n",
    "print(f\"False Positives: {false_positives.sum():,} (Normal classified as Attack)\")\n",
    "print(f\"False Negatives: {false_negatives.sum():,} (Attack classified as Normal)\")\n",
    "print(f\"\\nFalse Positive Rate: {false_positives.sum() / (all_labels == 0).sum():.4f}\")\n",
    "print(f\"False Negative Rate: {false_negatives.sum() / (all_labels == 1).sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence of misclassified samples\n",
    "fp_confidences = all_probs[false_positives, 1]  # Confidence in Attack class\n",
    "fn_confidences = all_probs[false_negatives, 0]  # Confidence in Normal class\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "if len(fp_confidences) > 0:\n",
    "    ax1.hist(fp_confidences, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax1.axvline(fp_confidences.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {fp_confidences.mean():.3f}')\n",
    "    ax1.set_xlabel('Confidence in Attack Prediction', fontsize=12)\n",
    "    ax1.set_ylabel('Count', fontsize=12)\n",
    "    ax1.set_title('False Positives - Prediction Confidence', fontsize=13, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "if len(fn_confidences) > 0:\n",
    "    ax2.hist(fn_confidences, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "    ax2.axvline(fn_confidences.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {fn_confidences.mean():.3f}')\n",
    "    ax2.set_xlabel('Confidence in Normal Prediction', fontsize=12)\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title('False Negatives - Prediction Confidence', fontsize=13, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'vqc_inference_error_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison with Classical Models\n",
    "\n",
    "Load results from classical models (if available) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load classical model results for comparison\n",
    "classical_results = {}\n",
    "\n",
    "# Check for CNN results\n",
    "cnn_metrics_files = sorted(LOGS_PATH.glob('cnn_*_metrics.json'))\n",
    "if cnn_metrics_files:\n",
    "    with open(cnn_metrics_files[-1], 'r') as f:\n",
    "        cnn_data = json.load(f)\n",
    "        if 'test_performance' in cnn_data:\n",
    "            classical_results['CNN'] = cnn_data['test_performance'].get('accuracy', 0) * 100\n",
    "\n",
    "# Check for LSTM results\n",
    "lstm_metrics_files = sorted(LOGS_PATH.glob('lstm_*_metrics.json'))\n",
    "if lstm_metrics_files:\n",
    "    with open(lstm_metrics_files[-1], 'r') as f:\n",
    "        lstm_data = json.load(f)\n",
    "        if 'test_performance' in lstm_data:\n",
    "            classical_results['LSTM'] = lstm_data['test_performance'].get('accuracy', 0) * 100\n",
    "\n",
    "# Check for Transformer results\n",
    "transformer_metrics_files = sorted(LOGS_PATH.glob('transformer_*_metrics.json'))\n",
    "if transformer_metrics_files:\n",
    "    with open(transformer_metrics_files[-1], 'r') as f:\n",
    "        transformer_data = json.load(f)\n",
    "        if 'test_performance' in transformer_data:\n",
    "            classical_results['Transformer'] = transformer_data['test_performance'].get('accuracy', 0) * 100\n",
    "\n",
    "# Add VQC results\n",
    "classical_results['VQC (Quantum)'] = accuracy * 100\n",
    "\n",
    "print(\"Model Comparison (Test Accuracy):\")\n",
    "for model_name, acc in classical_results.items():\n",
    "    print(f\"  {model_name:20s}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "if len(classical_results) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    models = list(classical_results.keys())\n",
    "    accuracies = list(classical_results.values())\n",
    "    \n",
    "    colors = ['steelblue'] * (len(models) - 1) + ['orange']  # Highlight VQC\n",
    "    bars = ax.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}%',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    ax.set_title('Model Comparison - NSL-KDD Binary Classification', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_PATH / 'vqc_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough models for comparison. Run classical models first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed inference results\n",
    "inference_results = {\n",
    "    'model_file': MODEL_FILE.name,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'test_samples': int(len(all_labels)),\n",
    "    'metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'roc_auc': float(roc_auc)\n",
    "    },\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'error_analysis': {\n",
    "        'false_positives': int(false_positives.sum()),\n",
    "        'false_negatives': int(false_negatives.sum()),\n",
    "        'false_positive_rate': float(false_positives.sum() / (all_labels == 0).sum()),\n",
    "        'false_negative_rate': float(false_negatives.sum() / (all_labels == 1).sum())\n",
    "    },\n",
    "    'inference_performance': {\n",
    "        'total_time_seconds': float(total_inference_time),\n",
    "        'avg_time_per_sample_ms': float(avg_time_per_sample * 1000),\n",
    "        'throughput_samples_per_second': float(len(all_preds) / total_inference_time)\n",
    "    },\n",
    "    'model_comparison': classical_results\n",
    "}\n",
    "\n",
    "results_file = LOGS_PATH / f'vqc_inference_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(inference_results, f, indent=2)\n",
    "\n",
    "print(f\"Inference results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VQC INFERENCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: {MODEL_FILE.name}\")\n",
    "print(f\"Test Samples: {len(all_labels):,}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"\\nInference Speed:\")\n",
    "print(f\"  Total time: {total_inference_time:.2f} seconds\")\n",
    "print(f\"  Per sample: {avg_time_per_sample*1000:.2f} ms\")\n",
    "print(f\"  Throughput: {len(all_preds)/total_inference_time:.2f} samples/sec\")\n",
    "print(f\"\\nErrors:\")\n",
    "print(f\"  False Positives: {false_positives.sum():,}\")\n",
    "print(f\"  False Negatives: {false_negatives.sum():,}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
