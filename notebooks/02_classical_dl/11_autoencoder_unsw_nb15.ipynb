{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Autoencoder for UNSW-NB15\n",
    "\n",
    "This notebook trains a **Semi-Supervised Autoencoder** for intrusion detection on the **UNSW-NB15** dataset.\n",
    "\n",
    "**Key Strategy:**\n",
    "- **Hybrid Approach**: The model is trained primarily on **Normal** data to learn reconstruction, but also includes a small fraction of **Attack** data with a contrastive loss.\n",
    "- **Objective**: Minimize reconstruction error for normal traffic while maximizing separation from known attacks.\n",
    "- **Advantage**: Better decision boundary than purely unsupervised methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path(\"../../\").resolve()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"latent_dim\": 32,\n",
    "    \"dropout\": 0.3,\n",
    "    \"patience\": 15,\n",
    "    \"recon_weight\": 0.3,\n",
    "    \"class_weight\": 0.7,\n",
    "}\n",
    "\n",
    "CATEGORICAL_COLS = ['proto', 'service', 'state']\n",
    "DROP_COLS = ['id', 'attack_cat']\n",
    "LABEL_COL = 'label'\n",
    "DATA_DIR = Path(\"../../data/raw/unsw-nb15\")\n",
    "\n",
    "def load_unsw_nb15(filepath, scaler=None, label_encoders=None, fit=True):\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    df = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors='ignore')\n",
    "    y = df[LABEL_COL].values\n",
    "    X = df.drop(columns=[LABEL_COL])\n",
    "    \n",
    "    if fit:\n",
    "        label_encoders = {}\n",
    "        for col in CATEGORICAL_COLS:\n",
    "            if col in X.columns:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = X[col].fillna('unknown').astype(str)\n",
    "                X[col] = le.fit_transform(X[col])\n",
    "                label_encoders[col] = le\n",
    "    else:\n",
    "        for col in CATEGORICAL_COLS:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].fillna('unknown').astype(str)\n",
    "                X[col] = X[col].apply(lambda x: x if x in label_encoders[col].classes_ else 'unknown')\n",
    "                if 'unknown' not in label_encoders[col].classes_:\n",
    "                    label_encoders[col].classes_ = np.append(label_encoders[col].classes_, 'unknown')\n",
    "                X[col] = label_encoders[col].transform(X[col])\n",
    "    \n",
    "    X = X.fillna(0).values.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    if fit:\n",
    "        scaler = MinMaxScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X = scaler.transform(X)\n",
    "        X = np.clip(X, 0, 1)\n",
    "        \n",
    "    return X, y, scaler, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load ALL training data\n",
    "    X_train_full, y_train_full, scaler, label_encoders = load_unsw_nb15(\n",
    "        DATA_DIR / \"UNSW_NB15_training-set.csv\", fit=True\n",
    "    )\n",
    "    \n",
    "    X_test, y_test, _, _ = load_unsw_nb15(\n",
    "        DATA_DIR / \"UNSW_NB15_testing-set.csv\", \n",
    "        scaler=scaler, label_encoders=label_encoders, fit=False\n",
    "    )\n",
    "    \n",
    "    # Prepare Semi-Supervised Data\n",
    "    # Use ALL Normal data + 30% of Attack data for training\n",
    "    normal_mask = (y_train_full == 0)\n",
    "    attack_mask = (y_train_full == 1)\n",
    "    \n",
    "    X_normal = X_train_full[normal_mask]\n",
    "    y_normal = y_train_full[normal_mask]\n",
    "    X_attack = X_train_full[attack_mask]\n",
    "    y_attack = y_train_full[attack_mask]\n",
    "    \n",
    "    n_attack_train = int(0.3 * len(X_attack))\n",
    "    indices = np.random.permutation(len(X_attack))\n",
    "    \n",
    "    X_attack_train = X_attack[indices[:n_attack_train]]\n",
    "    y_attack_train = y_attack[indices[:n_attack_train]]\n",
    "    \n",
    "    X_train = np.vstack([X_normal, X_attack_train])\n",
    "    y_train = np.concatenate([y_normal, y_attack_train])\n",
    "    \n",
    "    # Shuffle\n",
    "    shuffle_idx = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[shuffle_idx]\n",
    "    y_train = y_train[shuffle_idx]\n",
    "    \n",
    "    # Validation Split of Mixed Data\n",
    "    val_size = int(0.1 * len(X_train))\n",
    "    X_val = X_train[:val_size]\n",
    "    y_val = y_train[:val_size]\n",
    "    X_train = X_train[val_size:]\n",
    "    y_train = y_train[val_size:]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Attack samples in training: {n_attack_train:,}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Dataset not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "This autoencoder has a classification head attached to the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiSupervisedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], latent_dim=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim)\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Decoder\n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in reversed(hidden_dims):\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(dropout * 0.5)\n",
    "            ])\n",
    "            in_dim = h_dim\n",
    "        layers.extend([\n",
    "            nn.Linear(in_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Classifier Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_norm = self.input_bn(x)\n",
    "        z = self.encoder(x_norm)\n",
    "        x_recon = self.decoder(z)\n",
    "        logits = self.classifier(z)\n",
    "        return x_recon, z, logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if 'X_train' in globals():\n",
    "    model = SemiSupervisedAutoencoder(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dims=CONFIG[\"hidden_dims\"],\n",
    "        latent_dim=CONFIG[\"latent_dim\"],\n",
    "        dropout=CONFIG[\"dropout\"]\n",
    "    ).to(device)\n",
    "    print(f\"Model on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = torch.FloatTensor(len(y_train) / (2 * class_counts)).to(device)\n",
    "\n",
    "criterion_recon = nn.MSELoss()\n",
    "criterion_class = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train)), \n",
    "    batch_size=CONFIG[\"batch_size\"], shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val)), \n",
    "    batch_size=CONFIG[\"batch_size\"], shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test)), \n",
    "    batch_size=CONFIG[\"batch_size\"], shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_recon, z, logits = model(X_batch)\n",
    "        \n",
    "        loss_recon = criterion_recon(x_recon, X_batch)\n",
    "        loss_class = criterion_class(logits, y_batch)\n",
    "        loss = CONFIG[\"recon_weight\"] * loss_recon + CONFIG[\"class_weight\"] * loss_class\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = logits.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += pred.eq(y_batch).sum().item()\n",
    "        \n",
    "    return total_loss/len(loader), correct/total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            x_recon, z, logits = model(X_batch)\n",
    "            \n",
    "            loss_recon = criterion_recon(x_recon, X_batch)\n",
    "            loss_class = criterion_class(logits, y_batch)\n",
    "            loss = CONFIG[\"recon_weight\"] * loss_recon + CONFIG[\"class_weight\"] * loss_class\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, pred = logits.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += pred.eq(y_batch).sum().item()\n",
    "            \n",
    "    return total_loss/len(loader), correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in globals():\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_autoencoder_unsw.pt\")\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in globals():\n",
    "    model.load_state_dict(torch.load(\"best_autoencoder_unsw.pt\"))\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            _, _, logits = model(X_batch)\n",
    "            _, pred = logits.max(1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds, target_names=['Normal', 'Attack']))\n",
    "    sns.heatmap(confusion_matrix(all_labels, all_preds), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
