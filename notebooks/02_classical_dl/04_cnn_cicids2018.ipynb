{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CNN Classifier Training on CICIDS2018\n",
                "\n",
                "This notebook trains a 1D Convolutional Neural Network on the CICIDS2018 dataset.\n",
                "\n",
                "**Model:** CNNClassifier (PyTorch)  \n",
                "**Dataset:** CICIDS2018  \n",
                "**Task:** Binary Classification (Benign vs Attack)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '../..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import glob\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "import gc\n",
                "\n",
                "# Check GPU availability\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 10 files.\n",
                        "Loading 03-01-2018.csv...\n",
                        "Loading 02-28-2018.csv...\n",
                        "Loading 02-21-2018.csv...\n",
                        "Loading 02-16-2018.csv...\n",
                        "Loading 02-14-2018.csv...\n",
                        "Loading 02-22-2018.csv...\n",
                        "Loading 03-02-2018.csv...\n",
                        "Loading 02-15-2018.csv...\n",
                        "Loading 02-23-2018.csv...\n",
                        "Skipping 02-20-2018.csv (3867MB - too large)\n",
                        "Loaded 8,247,888 samples from 9 files\n",
                        "  Benign: 6,077,145\n",
                        "  Attack: 2,170,743\n"
                    ]
                }
            ],
            "source": [
                "# Load all CSV files from the CICIDS2018 raw directory\n",
                "DATA_PATH = '../../data/raw/cicids2018/'\n",
                "all_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n",
                "all_files = sorted(all_files, key=lambda x: os.path.getsize(x))\n",
                "\n",
                "print(f\"Found {len(all_files)} files.\")\n",
                "li = []\n",
                "\n",
                "for filename in all_files:\n",
                "    file_size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
                "    if file_size_mb > 1000:\n",
                "        print(f\"Skipping {os.path.basename(filename)} ({file_size_mb:.0f}MB - too large)\")\n",
                "        continue\n",
                "    print(f\"Loading {os.path.basename(filename)}...\")\n",
                "    try:\n",
                "        df_temp = pd.read_csv(filename, index_col=None, header=0, low_memory=True)\n",
                "        li.append(df_temp)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {filename}: {e}\")\n",
                "\n",
                "# Concatenate\n",
                "df = pd.concat(li, axis=0, ignore_index=True)\n",
                "print(f\"Loaded {len(df):,} samples from {len(li)} files\")\n",
                "\n",
                "# Garbage collection to free memory\n",
                "del li\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Samples after dropping NaN/Inf: 8,247,888\n",
                        "Remaining columns: 80\n"
                    ]
                }
            ],
            "source": [
                "# Basic Cleaning\n",
                "# 1. Strip whitespace from column names\n",
                "df.columns = df.columns.str.strip()\n",
                "\n",
                "# 2. Replace Inf with NaN and drop NaNs\n",
                "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "df.dropna(inplace=True)\n",
                "print(f\"Samples after dropping NaN/Inf: {len(df):,}\")\n",
                "print(\"Remaining columns:\", len(df.columns))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Binary class distribution:\n",
                        "0    6077145\n",
                        "1    2170743\n",
                        "Name: binary_label, dtype: int64\n",
                        "Sampled to 1,000,000 samples\n",
                        "X shape: (1000000, 76)\n"
                    ]
                }
            ],
            "source": [
                "# Create Binary Label: Benign (0) vs Attack (1)\n",
                "def create_binary_label(label):\n",
                "    if isinstance(label, str) and 'BENIGN' in label.upper():\n",
                "        return 0\n",
                "    return 1\n",
                "\n",
                "df['binary_label'] = df['Label'].apply(create_binary_label)\n",
                "\n",
                "print(\"Binary class distribution:\")\n",
                "print(df['binary_label'].value_counts())\n",
                "\n",
                "# Drop non-numeric columns\n",
                "drop_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Label']\n",
                "for col in drop_cols:\n",
                "    if col in df.columns:\n",
                "        df.drop(columns=[col], inplace=True, errors='ignore')\n",
                "\n",
                "# Convert all remaining columns to numeric\n",
                "for col in df.columns:\n",
                "    if col != 'binary_label':\n",
                "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
                "\n",
                "# Drop any new NaNs from conversion\n",
                "df.dropna(inplace=True)\n",
                "\n",
                "# Sample balanced data to manage memory\n",
                "SAMPLE_SIZE_PER_CLASS = 500000\n",
                "benign = df[df['binary_label'] == 0].sample(n=SAMPLE_SIZE_PER_CLASS, random_state=42)\n",
                "attack = df[df['binary_label'] == 1].sample(n=SAMPLE_SIZE_PER_CLASS, random_state=42)\n",
                "df = pd.concat([benign, attack], ignore_index=True)\n",
                "print(f\"Sampled to {len(df):,} samples\")\n",
                "\n",
                "# Separate features and target\n",
                "y = df['binary_label'].values\n",
                "X = df.drop(columns=['binary_label']).values\n",
                "\n",
                "# Free df memory\n",
                "del df\n",
                "gc.collect()\n",
                "\n",
                "print(f\"X shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train: 700,000\n",
                        "Val:   150,000\n",
                        "Test:  150,000\n"
                    ]
                }
            ],
            "source": [
                "# Split into Train, Val, Test\n",
                "# Stratified split to maintain class balance\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
                "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
                "\n",
                "print(f\"Train: {X_train.shape[0]:,}\")\n",
                "print(f\"Val:   {X_val.shape[0]:,}\")\n",
                "print(f\"Test:  {X_test.shape[0]:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Scaling\n",
                "scaler = StandardScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_val = scaler.transform(X_val)\n",
                "X_test = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train batches: 684\n"
                    ]
                }
            ],
            "source": [
                "# Create DataLoaders\n",
                "def create_loaders(X, y, batch_size=256):\n",
                "    # Reshape for CNN: (batch, channels, features) -> (batch, 1, n_features)\n",
                "    X_tensor = torch.FloatTensor(X).unsqueeze(1)\n",
                "    y_tensor = torch.LongTensor(y)\n",
                "    \n",
                "    dataset = TensorDataset(X_tensor, y_tensor)\n",
                "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
                "    return loader\n",
                "\n",
                "BATCH_SIZE = 1024 # Larger batch size for large dataset\n",
                "train_loader = create_loaders(X_train, y_train, BATCH_SIZE)\n",
                "val_loader = create_loaders(X_val, y_val, BATCH_SIZE)\n",
                "test_loader = create_loaders(X_test, y_test, BATCH_SIZE)\n",
                "\n",
                "print(f\"Train batches: {len(train_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Define CNN Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CNNClassifier(\n",
                        "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
                        "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
                        "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
                        "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (adaptive_pool): AdaptiveAvgPool1d(output_size=8)\n",
                        "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
                        "  (dropout1): Dropout(p=0.3, inplace=False)\n",
                        "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
                        "  (dropout2): Dropout(p=0.3, inplace=False)\n",
                        "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
                        "  (relu): ReLU()\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "class CNNClassifier(nn.Module):\n",
                "    def __init__(self, input_dim, num_classes=2, dropout_rate=0.3):\n",
                "        super(CNNClassifier, self).__init__()\n",
                "        \n",
                "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
                "        self.bn1 = nn.BatchNorm1d(64)\n",
                "        \n",
                "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
                "        self.bn2 = nn.BatchNorm1d(128)\n",
                "        \n",
                "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
                "        self.bn3 = nn.BatchNorm1d(256)\n",
                "        \n",
                "        self.adaptive_pool = nn.AdaptiveAvgPool1d(8)\n",
                "        \n",
                "        self.fc1 = nn.Linear(256 * 8, 256)\n",
                "        self.dropout1 = nn.Dropout(dropout_rate)\n",
                "        self.fc2 = nn.Linear(256, 64)\n",
                "        self.dropout2 = nn.Dropout(dropout_rate)\n",
                "        self.fc3 = nn.Linear(64, num_classes)\n",
                "        \n",
                "        self.relu = nn.ReLU()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.relu(self.bn1(self.conv1(x)))\n",
                "        x = self.relu(self.bn2(self.conv2(x)))\n",
                "        x = self.relu(self.bn3(self.conv3(x)))\n",
                "        \n",
                "        x = self.adaptive_pool(x)\n",
                "        x = x.view(x.size(0), -1)\n",
                "        \n",
                "        x = self.dropout1(self.relu(self.fc1(x)))\n",
                "        x = self.dropout2(self.relu(self.fc2(x)))\n",
                "        x = self.fc3(x)\n",
                "        return x\n",
                "\n",
                "input_dim = X_train.shape[1]\n",
                "model = CNNClassifier(input_dim=input_dim).to(device)\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loss with class weighting (Focal Loss)\n",
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, alpha=1, gamma=2):\n",
                "        super().__init__()\n",
                "        self.alpha = alpha\n",
                "        self.gamma = gamma\n",
                "        \n",
                "    def forward(self, inputs, targets):\n",
                "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
                "        pt = torch.exp(-ce_loss)\n",
                "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
                "        return focal_loss.mean()\n",
                "\n",
                "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
                "\n",
                "EPOCHS = 20\n",
                "EARLY_STOPPING_PATIENCE = 5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, criterion, optimizer):\n",
                "    model.train()\n",
                "    total_loss, correct, total = 0, 0, 0\n",
                "    \n",
                "    for X_batch, y_batch in loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(X_batch)\n",
                "        loss = criterion(outputs, y_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item() * X_batch.size(0)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        total += y_batch.size(0)\n",
                "        correct += (predicted == y_batch).sum().item()\n",
                "        \n",
                "    return total_loss / total, correct / total\n",
                "\n",
                "def validate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    total_loss, correct, total = 0, 0, 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for X_batch, y_batch in loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            outputs = model(X_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            \n",
                "            total_loss += loss.item() * X_batch.size(0)\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += y_batch.size(0)\n",
                "            correct += (predicted == y_batch).sum().item()\n",
                "            \n",
                "    return total_loss / total, correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n",
                        "Epoch 1/20 | Train Loss: 0.0089 Acc: 0.9540 | Val Loss: 0.0081 Acc: 0.9584 | Time: 64.8s\n",
                        "Epoch 2/20 | Train Loss: 0.0084 Acc: 0.9586 | Val Loss: 0.0080 Acc: 0.9603 | Time: 63.1s\n",
                        "Epoch 3/20 | Train Loss: 0.0082 Acc: 0.9594 | Val Loss: 0.0084 Acc: 0.9577 | Time: 63.1s\n",
                        "Epoch 4/20 | Train Loss: 0.0081 Acc: 0.9596 | Val Loss: 0.0078 Acc: 0.9610 | Time: 58.4s\n",
                        "Epoch 5/20 | Train Loss: 0.0080 Acc: 0.9598 | Val Loss: 0.0080 Acc: 0.9590 | Time: 70.5s\n",
                        "Epoch 6/20 | Train Loss: 0.0079 Acc: 0.9600 | Val Loss: 0.0078 Acc: 0.9608 | Time: 56.7s\n",
                        "Epoch 7/20 | Train Loss: 0.0078 Acc: 0.9604 | Val Loss: 0.0076 Acc: 0.9613 | Time: 63.5s\n",
                        "Epoch 8/20 | Train Loss: 0.0078 Acc: 0.9604 | Val Loss: 0.0076 Acc: 0.9613 | Time: 60.0s\n",
                        "Epoch 9/20 | Train Loss: 0.0077 Acc: 0.9608 | Val Loss: 0.0136 Acc: 0.9285 | Time: 89.4s\n",
                        "Epoch 10/20 | Train Loss: 0.0077 Acc: 0.9611 | Val Loss: 0.0076 Acc: 0.9599 | Time: 81.5s\n",
                        "Epoch 11/20 | Train Loss: 0.0077 Acc: 0.9615 | Val Loss: 0.0297 Acc: 0.8081 | Time: 59.0s\n",
                        "Epoch 12/20 | Train Loss: 0.0077 Acc: 0.9612 | Val Loss: 0.0076 Acc: 0.9638 | Time: 63.7s\n",
                        "Epoch 13/20 | Train Loss: 0.0077 Acc: 0.9616 | Val Loss: 0.0075 Acc: 0.9615 | Time: 62.5s\n",
                        "Epoch 14/20 | Train Loss: 0.0076 Acc: 0.9623 | Val Loss: 0.0075 Acc: 0.9621 | Time: 49.5s\n",
                        "Epoch 15/20 | Train Loss: 0.0075 Acc: 0.9625 | Val Loss: 0.0383 Acc: 0.8052 | Time: 43.5s\n",
                        "Epoch 16/20 | Train Loss: 0.0075 Acc: 0.9627 | Val Loss: 0.0073 Acc: 0.9634 | Time: 43.5s\n",
                        "Epoch 17/20 | Train Loss: 0.0075 Acc: 0.9627 | Val Loss: 0.0074 Acc: 0.9639 | Time: 45.3s\n",
                        "Epoch 18/20 | Train Loss: 0.0075 Acc: 0.9627 | Val Loss: 0.0074 Acc: 0.9615 | Time: 43.5s\n",
                        "Epoch 19/20 | Train Loss: 0.0075 Acc: 0.9627 | Val Loss: 0.0076 Acc: 0.9622 | Time: 44.8s\n",
                        "Epoch 20/20 | Train Loss: 0.0075 Acc: 0.9627 | Val Loss: 0.0073 Acc: 0.9640 | Time: 43.7s\n"
                    ]
                }
            ],
            "source": [
                "best_val_loss = float('inf')\n",
                "patience_counter = 0\n",
                "\n",
                "print(\"Starting training...\")\n",
                "for epoch in range(EPOCHS):\n",
                "    start_time = time.time()\n",
                "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
                "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
                "    \n",
                "    scheduler.step(val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | Time: {time.time()-start_time:.1f}s\")\n",
                "    \n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save(model.state_dict(), '../../results/models/best_cnn_cicids2018.pth')\n",
                "        patience_counter = 0\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
                "            print(\"Early stopping triggered.\")\n",
                "            break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "      Benign       0.94      0.99      0.97     75000\n",
                        "      Attack       0.99      0.94      0.96     75000\n",
                        "\n",
                        "    accuracy                           0.96    150000\n",
                        "   macro avg       0.97      0.96      0.96    150000\n",
                        "weighted avg       0.97      0.96      0.96    150000\n",
                        "\n",
                        "Confusion Matrix:\n",
                        "TN: 74,514 | FP: 486\n",
                        "FN: 4,863 | TP: 70,137\n",
                        "\n",
                        "Test Accuracy: 96.43%\n"
                    ]
                }
            ],
            "source": [
                "# Load best model\n",
                "model.load_state_dict(torch.load('../../results/models/best_cnn_cicids2018.pth', weights_only=True))\n",
                "model.eval()\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for X_batch, y_batch in test_loader:\n",
                "        X_batch = X_batch.to(device)\n",
                "        outputs = model(X_batch)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        all_preds.extend(predicted.cpu().numpy())\n",
                "        all_labels.extend(y_batch.numpy())\n",
                "\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(all_labels, all_preds, target_names=['Benign', 'Attack']))\n",
                "\n",
                "cm = confusion_matrix(all_labels, all_preds)\n",
                "print(\"Confusion Matrix:\")\n",
                "print(f\"TN: {cm[0][0]:,} | FP: {cm[0][1]:,}\")\n",
                "print(f\"FN: {cm[1][0]:,} | TP: {cm[1][1]:,}\")\n",
                "\n",
                "accuracy = (cm[0][0] + cm[1][1]) / cm.sum()\n",
                "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
                "\n",
                "torch.cuda.empty_cache()\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results saved!\n"
                    ]
                }
            ],
            "source": [
                "# Save results\n",
                "with open('../../results/cnn_cicids2018_results.txt', 'w') as f:\n",
                "    f.write(\"CNN CICIDS2018 Results\\n\")\n",
                "    f.write(\"=\" * 50 + \"\\n\")\n",
                "    f.write(classification_report(all_labels, all_preds, target_names=['Benign', 'Attack']))\n",
                "    f.write(f\"\\nConfusion Matrix:\\n{cm}\\n\")\n",
                "print(\"Results saved!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}