{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved LSTM for NSL-KDD (Binary Classification)\n",
    "\n",
    "This notebook trains an **Improved LSTM** model for intrusion detection on the **NSL-KDD** dataset.\n",
    "\n",
    "**Goal**: Achieve high accuracy in distinguishing **Normal** traffic from **Attacks**.\n",
    "\n",
    "**Key Features:**\n",
    "- **Bidirectional LSTM**: Captures context from both directions.\n",
    "- **Dropout & Batch Norm**: Regularization to prevent overfitting.\n",
    "- **Class Weights**: Handles class imbalance in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path(\"../../\").resolve()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.3,\n",
    "    \"patience\": 10,\n",
    "}\n",
    "\n",
    "COLUMNS = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'difficulty_level'\n",
    "]\n",
    "\n",
    "ATTACK_MAPPING = {\n",
    "    'normal': 0,\n",
    "    'back': 1, 'land': 1, 'neptune': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,\n",
    "    'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1,\n",
    "    'ipsweep': 1, 'nmap': 1, 'portsweep': 1, 'satan': 1, 'mscan': 1, 'saint': 1,\n",
    "    'ftp_write': 1, 'guess_passwd': 1, 'imap': 1, 'multihop': 1, 'phf': 1,\n",
    "    'spy': 1, 'warezclient': 1, 'warezmaster': 1, 'sendmail': 1, 'named': 1,\n",
    "    'snmpgetattack': 1, 'snmpguess': 1, 'xlock': 1, 'xsnoop': 1, 'worm': 1,\n",
    "    'buffer_overflow': 1, 'loadmodule': 1, 'perl': 1, 'rootkit': 1,\n",
    "    'httptunnel': 1, 'ps': 1, 'sqlattack': 1, 'xterm': 1,\n",
    "}\n",
    "\n",
    "DATA_PATH = Path(\"../../data/raw/nsl-kdd\")\n",
    "\n",
    "def load_nsl_kdd(filepath, scaler=None, label_encoders=None, fit=True):\n",
    "    df = pd.read_csv(filepath, header=None, names=COLUMNS)\n",
    "    print(f\"Loaded {len(df):,} samples from {filepath.name}\")\n",
    "    \n",
    "    df = df.drop('difficulty_level', axis=1)\n",
    "    df['label'] = df['label'].map(lambda x: ATTACK_MAPPING.get(x, 1))\n",
    "    \n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label'].values\n",
    "    \n",
    "    cat_cols = ['protocol_type', 'service', 'flag']\n",
    "    if fit:\n",
    "        label_encoders = {}\n",
    "        for col in cat_cols:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    else:\n",
    "        for col in cat_cols:\n",
    "            X[col] = X[col].apply(lambda x: x if x in label_encoders[col].classes_ else 'unknown')\n",
    "            if 'unknown' not in label_encoders[col].classes_:\n",
    "                label_encoders[col].classes_ = np.append(label_encoders[col].classes_, 'unknown')\n",
    "            X[col] = label_encoders[col].transform(X[col].astype(str))\n",
    "            \n",
    "    X = X.values.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    if fit:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X = scaler.transform(X)\n",
    "        \n",
    "    return X, y, scaler, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train, y_train, scaler, label_encoders = load_nsl_kdd(\n",
    "        DATA_PATH / \"train.txt\", fit=True\n",
    "    )\n",
    "    \n",
    "    X_test, y_test, _, _ = load_nsl_kdd(\n",
    "        DATA_PATH / \"test.txt\", \n",
    "        scaler=scaler, label_encoders=label_encoders, fit=False\n",
    "    )\n",
    "    \n",
    "    val_size = int(0.1 * len(X_train))\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    X_val = X_train[indices[:val_size]]\n",
    "    y_val = y_train[indices[:val_size]]\n",
    "    X_train = X_train[indices[val_size:]]\n",
    "    y_train = y_train[indices[val_size:]]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Dataset not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2, hidden_dim=256, num_layers=3, dropout=0.3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.input_bn = nn.BatchNorm1d(input_dim)\n",
    "        self.input_fc = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_dim * self.num_directions),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * self.num_directions, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_bn(x)\n",
    "        x = self.input_fc(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = x.unsqueeze(1) # (batch, 1, hidden)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :] # Last hidden state\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if 'X_train' in globals():\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        num_layers=CONFIG[\"num_layers\"],\n",
    "        dropout=CONFIG[\"dropout\"]\n",
    "    ).to(device)\n",
    "    print(f\"Model on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in globals():\n",
    "    # Loss with Class Weights\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = torch.FloatTensor(len(y_train) / (2 * class_counts)).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train)), \n",
    "        batch_size=CONFIG[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val)), \n",
    "        batch_size=CONFIG[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test)), \n",
    "        batch_size=CONFIG[\"batch_size\"], shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += pred.eq(y_batch).sum().item()\n",
    "    return total_loss/len(loader), correct/total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            _, pred = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += pred.eq(y_batch).sum().item()\n",
    "    return total_loss/len(loader), correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in globals():\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_lstm_kdd.pt\")\n",
    "            \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in globals():\n",
    "    model.load_state_dict(torch.load(\"best_lstm_kdd.pt\"))\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, pred = outputs.max(1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            \n",
    "    print(classification_report(all_labels, all_preds, target_names=['Normal', 'Attack']))\n",
    "    sns.heatmap(confusion_matrix(all_labels, all_preds), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
