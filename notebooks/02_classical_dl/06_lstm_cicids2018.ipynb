{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LSTM Classifier Training on CICIDS2018\n",
                "\n",
                "This notebook trains a Long Short-Term Memory (LSTM) network on the CICIDS2018 dataset.\n",
                "\n",
                "**Model:** LSTMClassifier (PyTorch)  \n",
                "**Dataset:** CICIDS2018  \n",
                "**Task:** Binary Classification (Benign vs Attack)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(str(Path(\"../../\").resolve()))\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import glob\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "from src.models.classical.lstm import LSTMClassifier\n",
                "import time\n",
                "import gc\n",
                "\n",
                "# Check GPU availability\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all CSV files from the CICIDS2018 raw directory\n",
                "DATA_PATH = '../../data/raw/cicids2018/'\n",
                "all_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n",
                "all_files = sorted(all_files, key=lambda x: os.path.getsize(x))\n",
                "\n",
                "print(f\"Found {len(all_files)} files.\")\n",
                "li = []\n",
                "\n",
                "for filename in all_files:\n",
                "    file_size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
                "    if file_size_mb > 1000:\n",
                "        print(f\"Skipping {os.path.basename(filename)} ({file_size_mb:.0f}MB - too large)\")\n",
                "        continue\n",
                "    print(f\"Loading {os.path.basename(filename)}...\")\n",
                "    try:\n",
                "        df_temp = pd.read_csv(filename, index_col=None, header=0, low_memory=True)\n",
                "        # Basic cleaning per file to save memory\n",
                "        df_temp.columns = df_temp.columns.str.strip()\n",
                "        drop_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Label']\n",
                "        label_col = df_temp['Label']\n",
                "        \n",
                "        # Keep label, drop metadata\n",
                "        df_temp = df_temp.drop(columns=[c for c in drop_cols if c in df_temp.columns and c != 'Label'], errors='ignore')\n",
                "        \n",
                "        li.append(df_temp)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {filename}: {e}\")\n",
                "\n",
                "# Concatenate\n",
                "df = pd.concat(li, axis=0, ignore_index=True)\n",
                "print(f\"Total raw samples: {len(df):,}\")\n",
                "del li\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocessing\n",
                "\n",
                "# 1. Create Binary Label\n",
                "def create_binary_label(label):\n",
                "    if isinstance(label, str) and 'BENIGN' in label.upper():\n",
                "        return 0\n",
                "    return 1\n",
                "\n",
                "df['binary_label'] = df['Label'].apply(create_binary_label)\n",
                "df.drop(columns=['Label'], inplace=True, errors='ignore')\n",
                "\n",
                "# 2. Convert to numeric\n",
                "for col in df.columns:\n",
                "    if col != 'binary_label':\n",
                "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
                "\n",
                "# 3. Clean NaNs/Infs\n",
                "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "df.dropna(inplace=True)\n",
                "\n",
                "print(\"Binary class distribution:\")\n",
                "print(df['binary_label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample data (LSTM is slow, so we sample)\n",
                "SAMPLE_SIZE_PER_CLASS = 200000\n",
                "benign = df[df['binary_label'] == 0].sample(n=min(len(df[df['binary_label']==0]), SAMPLE_SIZE_PER_CLASS), random_state=42)\n",
                "attack = df[df['binary_label'] == 1].sample(n=min(len(df[df['binary_label']==1]), SAMPLE_SIZE_PER_CLASS), random_state=42)\n",
                "\n",
                "df_sampled = pd.concat([benign, attack], ignore_index=True)\n",
                "print(f\"Sampled to {len(df_sampled):,} samples\")\n",
                "\n",
                "y = df_sampled['binary_label'].values\n",
                "X = df_sampled.drop(columns=['binary_label']).values.astype(np.float32)\n",
                "\n",
                "del df, df_sampled, benign, attack\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Data\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
                "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
                "\n",
                "# Scale Features\n",
                "scaler = StandardScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_val = scaler.transform(X_val)\n",
                "X_test = scaler.transform(X_test)\n",
                "\n",
                "# Create DataLoaders\n",
                "BATCH_SIZE = 512\n",
                "train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train)), batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val)), batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test)), batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Train batches: {len(train_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. LSTM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LSTMClassifier(\n",
                "    input_dim=X_train.shape[1],\n",
                "    num_classes=2,\n",
                "    lstm_units=[128, 64],\n",
                "    dense_units=[64],\n",
                "    dropout_rate=0.3\n",
                ").to(device)\n",
                "\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
                "\n",
                "def train_epoch(model, loader, criterion, optimizer):\n",
                "    model.train()\n",
                "    total_loss, correct, total = 0, 0, 0\n",
                "    for X_batch, y_batch in loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(X_batch)\n",
                "        loss = criterion(outputs, y_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item() * X_batch.size(0)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        total += y_batch.size(0)\n",
                "        correct += (predicted == y_batch).sum().item()\n",
                "    return total_loss / total, correct / total\n",
                "\n",
                "def validate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    total_loss, correct, total = 0, 0, 0\n",
                "    with torch.no_grad():\n",
                "        for X_batch, y_batch in loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            outputs = model(X_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            total_loss += loss.item() * X_batch.size(0)\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += y_batch.size(0)\n",
                "            correct += (predicted == y_batch).sum().item()\n",
                "    return total_loss / total, correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EPOCHS = 20\n",
                "best_val_acc = 0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
                "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
                "    scheduler.step(val_acc)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
                "    \n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        torch.save(model.state_dict(), \"../../results/models/best_lstm_cicids2018.pth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation\n",
                "model.load_state_dict(torch.load(\"../../results/models/best_lstm_cicids2018.pth\"))\n",
                "model.eval()\n",
                "\n",
                "all_preds, all_labels = [], []\n",
                "with torch.no_grad():\n",
                "    for X_batch, y_batch in test_loader:\n",
                "        X_batch = X_batch.to(device)\n",
                "        outputs = model(X_batch)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        all_preds.extend(predicted.cpu().numpy())\n",
                "        all_labels.extend(y_batch.numpy())\n",
                "\n",
                "print(classification_report(all_labels, all_preds, target_names=['Benign', 'Attack']))\n",
                "print(\"Confusion Matrix:\")\n",
                "print(confusion_matrix(all_labels, all_preds))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}