{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for UNSW-NB15 (Binary Classification)\n",
    "\n",
    "This notebook trains a **Transformer** model for binary intrusion detection on the **UNSW-NB15** dataset.\n",
    "\n",
    "**Key Features:**\n",
    "- **Self-Attention**: Uses Multi-Head Self-Attention to capture complex relationships between features.\n",
    "- **Architecture**: Encoder-only Transformer architecture adapted for tabular data.\n",
    "- **Goal**: Classify traffic as **Normal** (0) or **Attack** (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path(\"../../\").resolve()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Import Transformer Model\n",
    "from src.models.classical.transformer import TransformerClassifier\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"embed_dim\": 64,\n",
    "    \"num_heads\": 4,\n",
    "    \"ff_dim\": 128,\n",
    "    \"num_blocks\": 3,\n",
    "    \"dense_units\": [64],\n",
    "    \"dropout\": 0.3,\n",
    "    \"patience\": 10,\n",
    "}\n",
    "\n",
    "CATEGORICAL_COLS = ['proto', 'service', 'state']\n",
    "DROP_COLS = ['id', 'attack_cat']\n",
    "LABEL_COL = 'label'\n",
    "\n",
    "DATA_DIR = Path(\"../../data/raw/unsw-nb15\")\n",
    "\n",
    "def load_unsw_nb15(filepath, scaler=None, label_encoders=None, fit=True):\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "        \n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Loaded {len(df):,} samples from {filepath.name}\")\n",
    "    \n",
    "    df = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors='ignore')\n",
    "    \n",
    "    y = df[LABEL_COL].values\n",
    "    X = df.drop(columns=[LABEL_COL])\n",
    "    \n",
    "    if fit:\n",
    "        label_encoders = {}\n",
    "        for col in CATEGORICAL_COLS:\n",
    "            if col in X.columns:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = X[col].fillna('unknown').astype(str)\n",
    "                X[col] = le.fit_transform(X[col])\n",
    "                label_encoders[col] = le\n",
    "    else:\n",
    "        for col in CATEGORICAL_COLS:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].fillna('unknown').astype(str)\n",
    "                X[col] = X[col].apply(lambda x: x if x in label_encoders[col].classes_ else 'unknown')\n",
    "                if 'unknown' not in label_encoders[col].classes_:\n",
    "                    label_encoders[col].classes_ = np.append(label_encoders[col].classes_, 'unknown')\n",
    "                X[col] = label_encoders[col].transform(X[col])\n",
    "    \n",
    "    X = X.fillna(0)\n",
    "    X_vals = X.values.astype(np.float32)\n",
    "    X_vals = np.nan_to_num(X_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    if fit:\n",
    "        scaler = StandardScaler()\n",
    "        X_vals = scaler.fit_transform(X_vals)\n",
    "    else:\n",
    "        X_vals = scaler.transform(X_vals)\n",
    "        \n",
    "    return X_vals, y, scaler, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load Data\n",
    "    X_train, y_train, scaler, label_encoders = load_unsw_nb15(\n",
    "        DATA_DIR / \"UNSW_NB15_training-set.csv\", fit=True\n",
    "    )\n",
    "    \n",
    "    X_test, y_test, _, _ = load_unsw_nb15(\n",
    "        DATA_DIR / \"UNSW_NB15_testing-set.csv\",\n",
    "        scaler=scaler, label_encoders=label_encoders, fit=False\n",
    "    )\n",
    "    \n",
    "    val_size = int(0.1 * len(X_train))\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    X_val = X_train[indices[:val_size]]\n",
    "    y_val = y_train[indices[:val_size]]\n",
    "    X_train = X_train[indices[val_size:]]\n",
    "    y_train = y_train[indices[val_size:]]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Dataset not found! Please download UNSW-NB15 CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if 'X_train' in globals():\n",
    "    model = TransformerClassifier(\n",
    "        input_dim=X_train.shape[1],\n",
    "        num_classes=2,\n",
    "        embed_dim=CONFIG[\"embed_dim\"],\n",
    "        num_heads=CONFIG[\"num_heads\"],\n",
    "        ff_dim=CONFIG[\"ff_dim\"],\n",
    "        num_blocks=CONFIG[\"num_blocks\"],\n",
    "        dense_units=CONFIG[\"dense_units\"],\n",
    "        dropout=CONFIG[\"dropout\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model on {device}\")\n",
    "\n",
    "    # Class weights\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = torch.FloatTensor(len(y_train) / (2 * class_counts)).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    val_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "    test_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=CONFIG[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += pred.eq(y_batch).sum().item()\n",
    "    return total_loss/len(loader), correct/total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            _, pred = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += pred.eq(y_batch).sum().item()\n",
    "    return total_loss/len(loader), correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in globals():\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_transformer_unsw.pt\")\n",
    "            \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in globals():\n",
    "    model.load_state_dict(torch.load(\"best_transformer_unsw.pt\"))\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, pred = outputs.max(1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            \n",
    "    print(classification_report(all_labels, all_preds, target_names=['Normal', 'Attack']))\n",
    "    sns.heatmap(confusion_matrix(all_labels, all_preds), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
