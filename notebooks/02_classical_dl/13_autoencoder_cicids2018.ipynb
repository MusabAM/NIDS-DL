{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Autoencoder Training on CICIDS2018\n",
                "\n",
                "This notebook trains an **Autoencoder** for unsupervised anomaly detection on the CICIDS2018 dataset.\n",
                "\n",
                "**Model:** Autoencoder (PyTorch)  \n",
                "**Dataset:** CICIDS2018  \n",
                "**Task:** Anomaly Detection (Train on Benign, Detect Attacks)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(str(Path(\"../../\").resolve()))\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import glob\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from src.models.classical.autoencoder import Autoencoder\n",
                "import time\n",
                "import gc\n",
                "\n",
                "# Check GPU availability\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all CSV files from the CICIDS2018 raw directory\n",
                "DATA_PATH = '../../data/raw/cicids2018/'\n",
                "all_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n",
                "all_files = sorted(all_files, key=lambda x: os.path.getsize(x))\n",
                "\n",
                "print(f\"Found {len(all_files)} files.\")\n",
                "li = []\n",
                "\n",
                "for filename in all_files:\n",
                "    file_size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
                "    if file_size_mb > 1000:\n",
                "        print(f\"Skipping {os.path.basename(filename)} ({file_size_mb:.0f}MB - too large)\")\n",
                "        continue\n",
                "    print(f\"Loading {os.path.basename(filename)}...\")\n",
                "    try:\n",
                "        df_temp = pd.read_csv(filename, index_col=None, header=0, low_memory=True)\n",
                "        # Basic cleaning per file to save memory\n",
                "        df_temp.columns = df_temp.columns.str.strip()\n",
                "        drop_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Label']\n",
                "        label_col = df_temp['Label']\n",
                "        \n",
                "        # Keep label, drop metadata\n",
                "        df_temp = df_temp.drop(columns=[c for c in drop_cols if c in df_temp.columns and c != 'Label'], errors='ignore')\n",
                "        \n",
                "        li.append(df_temp)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {filename}: {e}\")\n",
                "\n",
                "# Concatenate\n",
                "df = pd.concat(li, axis=0, ignore_index=True)\n",
                "print(f\"Total raw samples: {len(df):,}\")\n",
                "del li\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocessing\n",
                "\n",
                "# 1. Create Binary Label\n",
                "def create_binary_label(label):\n",
                "    if isinstance(label, str) and 'BENIGN' in label.upper():\n",
                "        return 0\n",
                "    return 1\n",
                "\n",
                "df['binary_label'] = df['Label'].apply(create_binary_label)\n",
                "df.drop(columns=['Label'], inplace=True, errors='ignore')\n",
                "\n",
                "# 2. Convert to numeric\n",
                "for col in df.columns:\n",
                "    if col != 'binary_label':\n",
                "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
                "\n",
                "# 3. Clean NaNs/Infs\n",
                "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "df.dropna(inplace=True)\n",
                "\n",
                "print(\"Binary class distribution:\")\n",
                "print(df['binary_label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample data\n",
                "# For Autoencoder, we need primarily BENIGN data for training\n",
                "NORMAL_SAMPLES = 500000\n",
                "ATTACK_SAMPLES = 200000\n",
                "\n",
                "benign = df[df['binary_label'] == 0].sample(n=min(len(df[df['binary_label']==0]), NORMAL_SAMPLES), random_state=42)\n",
                "attack = df[df['binary_label'] == 1].sample(n=min(len(df[df['binary_label']==1]), ATTACK_SAMPLES), random_state=42)\n",
                "\n",
                "df_sampled = pd.concat([benign, attack], ignore_index=True)\n",
                "print(f\"Sampled to {len(df_sampled):,} samples\")\n",
                "\n",
                "y = df_sampled['binary_label'].values\n",
                "X = df_sampled.drop(columns=['binary_label']).values.astype(np.float32)\n",
                "\n",
                "del df, df_sampled, benign, attack\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data/Splits for Anomaly Detection\n",
                "# Train on Benign ONLY\n",
                "normal_indices = np.where(y == 0)[0]\n",
                "attack_indices = np.where(y == 1)[0]\n",
                "\n",
                "X_normal = X[normal_indices]\n",
                "X_attack = X[attack_indices]\n",
                "\n",
                "# Split Benign into Train (70%), Val (15%), Test (15%)\n",
                "X_train, X_temp = train_test_split(X_normal, test_size=0.3, random_state=42)\n",
                "X_val, X_test_normal = train_test_split(X_temp, test_size=0.5, random_state=42)\n",
                "\n",
                "# Test Set = Normal Test + All Attacks\n",
                "X_test = np.concatenate([X_test_normal, X_attack])\n",
                "y_test = np.concatenate([np.zeros(len(X_test_normal)), np.ones(len(X_attack))])\n",
                "\n",
                "# Scale Features (MinMax is often better for Autoencoders)\n",
                "scaler = MinMaxScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_val = scaler.transform(X_val)\n",
                "X_test = scaler.transform(X_test)\n",
                "\n",
                "# Create DataLoaders\n",
                "BATCH_SIZE = 512\n",
                "# Train loader only needs X (unsupervised)\n",
                "train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.zeros(len(X_train))), batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val), torch.zeros(len(X_val))), batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test)), batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Train samples: {len(X_train):,} (Normal)\")\n",
                "print(f\"Val samples: {len(X_val):,} (Normal)\")\n",
                "print(f\"Test samples: {len(X_test):,} (Mixed)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Autoencoder Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Autoencoder(\n",
                "    input_dim=X_train.shape[1],\n",
                "    encoder_units=[64, 32, 16],\n",
                "    latent_dim=8,\n",
                "    decoder_units=[16, 32, 64],\n",
                "    dropout_rate=0.2\n",
                ").to(device)\n",
                "\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.MSELoss()\n",
                "\n",
                "def train_epoch(model, loader, optimizer):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    for X_batch, _ in loader:\n",
                "        X_batch = X_batch.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        recon, _ = model(X_batch)\n",
                "        loss = criterion(recon, X_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item() * X_batch.size(0)\n",
                "    return total_loss / len(loader.dataset)\n",
                "\n",
                "def validate(model, loader):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for X_batch, _ in loader:\n",
                "            X_batch = X_batch.to(device)\n",
                "            recon, _ = model(X_batch)\n",
                "            loss = criterion(recon, X_batch)\n",
                "            total_loss += loss.item() * X_batch.size(0)\n",
                "    return total_loss / len(loader.dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EPOCHS = 30\n",
                "best_val_loss = float('inf')\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    train_loss = train_epoch(model, train_loader, optimizer)\n",
                "    val_loss = validate(model, val_loader)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f}\")\n",
                "    \n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save(model.state_dict(), \"../../results/models/best_autoencoder_cicids2018.pth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation\n",
                "model.load_state_dict(torch.load(\"../../results/models/best_autoencoder_cicids2018.pth\"))\n",
                "model.eval()\n",
                "\n",
                "# 1. Set Threshold on Validation Set\n",
                "model.set_threshold(torch.FloatTensor(X_val).to(device), percentile=95)\n",
                "print(f\"Anomaly Threshold: {model.threshold:.6f}\")\n",
                "\n",
                "# 2. Predict on Test Set\n",
                "all_preds, all_labels, all_errors = [], [], []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for X_batch, y_batch in test_loader:\n",
                "        X_batch = X_batch.to(device)\n",
                "        preds = model.predict(X_batch)\n",
                "        errors = model.reconstruction_error(X_batch)\n",
                "        \n",
                "        all_preds.extend(preds.cpu().numpy())\n",
                "        all_labels.extend(y_batch.numpy())\n",
                "        all_errors.extend(errors.cpu().numpy())\n",
                "\n",
                "print(classification_report(all_labels, all_preds, target_names=['Benign', 'Attack']))\n",
                "print(\"Confusion Matrix:\")\n",
                "print(confusion_matrix(all_labels, all_preds))\n",
                "\n",
                "roc_auc = roc_auc_score(all_labels, all_errors)\n",
                "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}